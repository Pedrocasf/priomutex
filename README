priomutex:  a mutex where waiting threads specify a priority
============================================================

The API is very similar to std::sync::Mutex.  The notable exception is that
lock() takes a priority.  If multiple threads are waiting for the mutex when
it's freed, the one which gave the highest priorty will recieve it.

    impl<T> Mutex<T> {
        fn new(data: T) -> Mutex<T>;
        fn lock(&self, prio: usize) -> MutexGuard<T>;
        fn try_lock(&self) -> Option<MutexGuard<T>>;
    }

    impl<T: Send> Send for Mutex<T>;
    impl<T> Clone for Mutex<T>;

    impl<T> Drop for MutexGuard<T>;      // Releases the lock
    impl<T> Deref for MutexGuard<T>;     // For accessing your data
    impl<T> DerefMut for MutexGuard<T>;  // For accessing your data

The other slight difference is that std::sync::Mutex implements Sync but not
Clone, whereas priomutex::Mutex implements Clone but not Sync.  In practice
this means that you don't need to wrap a priomutex in an Arc.

Current status
--------------

Currently, priomutexes are *not* poisoned if the thread holding the lock
panics.  I intend to add this safety feature at some point.

There's a bug which means that, in rare cases, a waiting thread may become
deadlocked.  Triggering this bug requires spawning billions of threads, so it's
unlikely to show up in normal code;  however, it may be possible for an
attacker to exploit it.  (See "Hash collision bug" for details.)

Implementation
--------------

At first I assumed that implementing priomutex would require a concurrent
priority queue, and was not looking forward to this.  However, it turns out it
doesn't need one!

When a thread fails to take the lock, it places a priority and a semaphore onto
an (ordinary, non-priority) MPSC queue, and then sleeps on the semaphore.  When
the thread holding the lock releases it, it first drains the concurrent queue
into a (ordinary, non-concurrent) binary heap, and pops out the semaphore with
the highest associated priority.  It then stores the remains of the heap behind
the lock, alongside the user's data. Finally, it releases the lock and signals
the popped semaphore.

Hash collision bug
------------------

You will need two threads (A and B) whose ThreadIds hash to the same usize.
Lock the mutex, and have A wait on it.  Release the lock.  (At this point the
lock is earmarked for A, and A's semaphore is signalled.)  Before A is woken by
the OS scheduler, take the lock on B.  Because their ThreadIds hash to the same
value, B will successfully take the lock.  When A wakes up, it will see that
the lock is taken, assume its wakeup was spurious, and go back to sleep;
however, A's semaphore is gone from the queue, meaning that A is now
deadlocked.

Triggering this issue requires spawning threads until a hash collision is
found.  On a 32-bit machine, you're guaranteed to find two such threads within
4.3 billion threads.  On some systems, this may only take 12 hours or so.
Given our hash algorithm and the current implementation of ThreadId, it will
actually take the full 4.3 billion before you see a collision, but this is not
guaranteed to be the case given a future version of libstd (>1.23).

I can't think of a proper fix right now which doesn't break the ThreadId
abstraction.  Breaking this abstraction would make priomutex unstable in the
case of upgrading libstd.

Licence
-------

Licensed under either of the following, at your option:

 * Apache Licence 2.0 (see LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0)
 * MIT licence (see LICENSE-MIT or http://opensource.org/licenses/MIT)

Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in the work by you, as defined in the Apache-2.0 license, shall
be dual licensed as above, without any additional terms or conditions.
